{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for knn model using k=7, considering all terms and using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[196  59]\n",
      "  [  4 141]]\n",
      "\n",
      " [[141   4]\n",
      "  [ 59 196]]]\n",
      "precision per class:     {'ham': 0.705, 'spam': 0.98}\n",
      "recall per class:        {'ham': 0.9724137931034482, 'spam': 0.7686274509803922}\n",
      "f1 per class             {'ham': 0.8173913043478261, 'spam': 0.8615384615384616}\n",
      "error rate per class     {'ham': 0.1575, 'spam': 0.1575}\n",
      "\n",
      "accuracy:                0.8425\n",
      "f1 macro:                0.8394648829431439\n",
      "f1 micro:                0.8425\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_simple.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones\n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized, similarity_func=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for knn model using k=7, considering all terms and using tf-idf scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[193  68]\n",
      "  [  7 132]]\n",
      "\n",
      " [[132   7]\n",
      "  [ 68 193]]]\n",
      "precision per class:     {'ham': 0.66, 'spam': 0.965}\n",
      "recall per class:        {'ham': 0.9496402877697842, 'spam': 0.7394636015325671}\n",
      "f1 per class             {'ham': 0.7787610619469026, 'spam': 0.8373101952277657}\n",
      "error rate per class     {'ham': 0.1875, 'spam': 0.1875}\n",
      "\n",
      "accuracy:                0.8125\n",
      "f1 macro:                0.8080356285873342\n",
      "f1 micro:                0.8125\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_simple.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones    \n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized, similarity_func=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for naive bayes, considering all terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[193  21]\n",
      "  [  7 179]]\n",
      "\n",
      " [[179   7]\n",
      "  [ 21 193]]]\n",
      "precision per class:     {'ham': 0.895, 'spam': 0.965}\n",
      "recall per class:        {'ham': 0.9623655913978495, 'spam': 0.9018691588785047}\n",
      "f1 per class             {'ham': 0.927461139896373, 'spam': 0.9323671497584541}\n",
      "error rate per class     {'ham': 0.07, 'spam': 0.07}\n",
      "\n",
      "accuracy:                0.93\n",
      "f1 macro:                0.9299141448274135\n",
      "f1 micro:                0.93\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_simple.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones\n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized,naive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for knn model using k=7, considering important terms and using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[195  66]\n",
      "  [  5 134]]\n",
      "\n",
      " [[134   5]\n",
      "  [ 66 195]]]\n",
      "precision per class:     {'ham': 0.67, 'spam': 0.975}\n",
      "recall per class:        {'ham': 0.9640287769784173, 'spam': 0.7471264367816092}\n",
      "f1 per class             {'ham': 0.7905604719764012, 'spam': 0.8459869848156182}\n",
      "error rate per class     {'ham': 0.1775, 'spam': 0.1775}\n",
      "\n",
      "accuracy:                0.8225\n",
      "f1 macro:                0.8182737283960098\n",
      "f1 micro:                0.8225\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_advance.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones\n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized, similarity_func=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for knn model using k=7, considering important terms and using tf-idf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[191  22]\n",
      "  [  9 178]]\n",
      "\n",
      " [[178   9]\n",
      "  [ 22 191]]]\n",
      "precision per class:     {'ham': 0.89, 'spam': 0.955}\n",
      "recall per class:        {'ham': 0.9518716577540107, 'spam': 0.8967136150234741}\n",
      "f1 per class             {'ham': 0.9198966408268734, 'spam': 0.9249394673123487}\n",
      "error rate per class     {'ham': 0.0775, 'spam': 0.0775}\n",
      "\n",
      "accuracy:                0.9225\n",
      "f1 macro:                0.922418054069611\n",
      "f1 micro:                0.9225\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_advance.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones  \n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized, similarity_func=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see evaluation result for naive bayes, considering important terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indexes: {'ham': 0, 'spam': 1}\n",
      "confusion_matrix:    \n",
      "[[[192  17]\n",
      "  [  8 183]]\n",
      "\n",
      " [[183   8]\n",
      "  [ 17 192]]]\n",
      "precision per class:     {'ham': 0.915, 'spam': 0.96}\n",
      "recall per class:        {'ham': 0.9581151832460733, 'spam': 0.9186602870813397}\n",
      "f1 per class             {'ham': 0.9360613810741688, 'spam': 0.9388753056234719}\n",
      "error rate per class     {'ham': 0.0625, 'spam': 0.0625}\n",
      "\n",
      "accuracy:                0.9375\n",
      "f1 macro:                0.9374683433488203\n",
      "f1 micro:                0.9375\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# import functions fromother .py files\n",
    "from constants import given_doc_root_path, document_root_path \n",
    "from preprocessing import prepare_data,construct_vocabulary,vectorize_data\n",
    "from model import evaluation\n",
    "\n",
    "# ham  and spam directory path for training data\n",
    "ham_train_dir = given_doc_root_path + 'train\\\\hamtraining\\\\'\n",
    "spam_train_dir = given_doc_root_path + 'train\\\\spamtraining\\\\'\n",
    "\n",
    "# process training data\n",
    "processed_train_dict = prepare_data(ham_train_dir, spam_train_dir)\n",
    "\n",
    "# ham  and spam directory path for testing data\n",
    "ham_test_dir = given_doc_root_path + 'test\\\\hamtesting\\\\'\n",
    "spam_test_dir = given_doc_root_path + 'test\\\\spamtesting\\\\'\n",
    "\n",
    "# process testing data\n",
    "processed_test_dict = prepare_data(ham_test_dir, spam_test_dir, False)\n",
    "\n",
    "vocabulary = dict()\n",
    "exist = False\n",
    "# sotre related dictionary into memory by using it's path (construct it if its not on the disk)\n",
    "while not exist:\n",
    "    try: \n",
    "        with open(document_root_path + 'vocabulary_advance.pickle', 'rb') as vocabulary_file:\n",
    "            vocabulary = pickle.load(vocabulary_file, encoding='utf8')\n",
    "            if len(vocabulary)>0:\n",
    "                exist = True\n",
    "    except IOError:\n",
    "        construct_vocabulary(processed_train_dict)\n",
    "\n",
    "# vectorize training and testing data using processed ones\n",
    "train_vectorized = vectorize_data(processed_train_dict, vocabulary)\n",
    "test_vectorized = vectorize_data(processed_test_dict, vocabulary)\n",
    "\n",
    "# call evaluation function for this mode\n",
    "evaluation(test_vectorized, train_vectorized,naive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
